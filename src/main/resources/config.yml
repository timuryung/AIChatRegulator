llmModelPath: "default/path/to/model"
systemPrompt: "Your task is to scrutinize messages from a chat environment to identify any unwanted or harmful behavior. Each message you receive should be carefully analyzed for content that may be considered offensive, threatening, discriminatory, or otherwise inappropriate. Your responses must be clear and straightforward. If a message is deemed acceptable and does not violate any standards of respectful and safe communication, respond ONLY with 'OK'. If a message is identified as harmful, which includes but is not limited to bullying, hate speech, harassment, or any form of aggressive or inappropriate conduct, respond with ONLY 'HARMFUL' and explain the reason. Remember, your assessment should strictly categorize each message as either 'OK' or 'HARMFUL' + reason, based on the content and context of the message. Message: "
warning_message: "§cWarning: This message was flagged as harmful by the AI system. Reason: "
startup_message: "LLM API server has been started."
llm_startup_error_message: "Failed to start LLM API server: "
warning_message_template: "§9Message : "
llm_response_message: "LLM API server response content: "
no_content_error_message: "No content found in LLM API server response."
api_error_message: "Error sending message to LLM API server: "
model_already_loaded_message: "Model already loaded."
failed_to_load_model_message: "Failed to load model."
api_fail_message: "Failed to send request to LLM API server: "
shutdown_message: "LLM API server has been stopped."
model_not_loaded_message: "§cModel is not loaded yet. Please wait..."
messages:
  inappropriateMessageCount: "§6%playerName% has %count% inappropriate messages."
